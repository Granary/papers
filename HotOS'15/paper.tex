%--------sigalternate----------
\documentclass{sig-alternate}
%--------/sigalternate----------

%\documentclass[letterpaper,twocolumn,10pt]{article}
%\usepackage{endnotes,multirow}
\usepackage{refstyle,amsmath,chngcntr}
\usepackage{epsfig,subfigure,framed}
\usepackage{textcomp} % for tilde
\usepackage{paralist} % for in-paragraph lists
\usepackage{graphicx}
\usepackage{hyperref}

% Used for code snippets
\usepackage{listings,courier}

% Make text in figure captions small
\usepackage{caption3} % load caption package kernel first
%\DeclareCaptionOption{parskip}[]{} % disable "parskip" caption option
\usepackage{caption}

% Make sure that figure numbers are continuous through the document
\counterwithout{figure}{section}
\counterwithout{figure}{subsection}

% Settings on code listings.
\lstset{language=C,
		xleftmargin=0pt,
		xrightmargin=0pt,
		framexbottommargin=0pt,
        framextopmargin=0pt,
        framesep=0pt}

% Modify spacing before/after title, sections, and subsections.
%\setlength{\droptitle}{-3em}  %--------sigalternate----------
%\posttitle{\par\end{center}\vspace{-1.5em}} %--------sigalternate----------
\newcommand{\Sref}[1]{Section~\ref{#1}} 

%--------sigalternate----------\titlespacing\section{0pt}{5pt plus 4pt minus 2pt}{5pt plus 2pt minus 2pt}        
%--------sigalternate----------\titlespacing\subsection{0pt}{5pt plus 4pt minus 2pt}{5pt plus 2pt minus 2pt}   

% Add a horizontal rule above captions, and make captions closer to their figures
\DeclareCaptionFormat{ruled_caption}{#1#2#3\hrulefill}
\captionsetup[figure]{format=ruled_caption}

% Prevent footnotes from spanning multiple pages
\interfootnotelinepenalty=10000

% For leaving some comments in the draft.
\newcommand{\comment}[1]{}
\newcommand{\Toolname}{DataReactor}

% For getting rid of copyright box
\makeatletter
\def\@copyrightspace{\relax}
\makeatother

\begin{document}

%make title bold and 14 pt font (Latex default is non-bold, 16 pt)
\title{\Large \bf \Toolname: Precise False Sharing Detection for COTS Binaries}
\numberofauthors{3}
\author{
% 1st. author
\alignauthor
Peter Goodman\\
       \email{pag@cs.toronto.edu}
\and
% 2nd. author
\alignauthor
Ashvin Goel\\
       \email{asvhin@eecg.toronto.edu}
% 3rd. author
\alignauthor
Angela Demke Brown\\
       \email{demke@cs.toronto.edu}
}
\maketitle

%\begin{abstract}
%\end{abstract}

\section{Introduction}\label{sec:intro}

% Key ideas:
%		1) 	Shadow memory as a proxy for memory accesses, not as a meta-data store.
%		2) 	Tagged reads as a way to avoid cache contention on shadow memory, but still
%			highlight the essential memory access behaviour.
%		3)	Proportional heap sampling, by using type-specific information to decide where
%			to place hardware watchpoints.

False-sharing is a performance-degrading problem that limits the scalability of multithreaded applications
\cite{ImpactOfFalseSharing}. This problem is increasingly important in the multi-core era. The level of
concurrency in applications is growing to match the additional processing power provided by increasing
core counts on modern processors. Despite this rise in importance, detecting and diagnosing false
sharing in large systems remains challenging. Due to the nature of cache coherent multiprocessors, false
sharing is often indistinguishable from true sharing (including data races). Also, diagnosing the cause
of false sharing requires that one distinguish accesses to the same objects from ``pseudo sharing", which
occurs when two nearby objects are separately but concurrently accessed \cite{DegenerateSharingAndFalseCoherence}.

In this work, we present \Toolname{}, a system for precisely detecting and diagnosing false sharing
in multi-threaded applications. \Toolname{} uses dynamic binary translation (DBT) and shadow memory
to detect false sharing in commercial off-the-shelf (COTS) binaries.  The key insight of our
approach is that accesses to shadow memory can be used as a proxy for accesses to actual memory. By 
detecting contention on proxied accesses, we can establish beliefs about what code causes false sharing. These
beliefs are strenghtened and eventually verified by specializing code to proxy memory accesses at increasingly
finer granularities.

Our work addresses three limitations of prior work:
\begin{enumerate}
\item They introduce extra cache coherency traffic, which limits scalability and hides performance
problems. The extra traffic introduced comes from the mechanisms used to track cache line ownership.

\item They are unable to selectively detect false sharing on only specific types of objects or specific
code.

\item They are imprecise: they cannot distinguish between false sharing, true sharing (including
data races), and and pseudo sharing. This can result in high false-positive rates that decrease the
usefulness of the tools.

\end{enumerate}

\paragraph{Contributions}
This paper makes the following significant contributions:
\begin{itemize}
\item It presents \Toolname{}, a software-only framework that dynamically and precisely diagnoses
false sharing  in multi-threaded, COTS binaries. We believe that operating directly on binaries is
important because it allows one to verify observed performance problems using the same binaries
that are running in a production environment.

\item It presents two new techniques, \emph{proxy memory} and \emph{tagged reads}, that enable
efficient determination of memory access behavior using shadow memory. Unlike existing shadow
memory systems, proxy memory is not treated as a mutable meta-data store. This avoids extra
serialization and coherence traffic introduced by prior work when dynamically manipulating shadow state.

\item It describes a two new, data-centric sampling approaches: \begin{inparaenum}[i)]
\item uniform heap sampling, and
\item selective heap sampling.
\end{inparaenum} Uniform heap sampling enables \Toolname{} to sample objects in the heap uniformly
according to their type. This is beneficial because it improves coverage: if 90\% of false sharing occurs because
of accesses to only 10\% of the data, then \Toolname{} will not miss those races by blindly focusing on the
other 90\% of data. Once data of interesting has been narrowed down, \Toolname{} can apply selective heap
sampling, which focuses sampling on specific types of objects or data accessed by specific code (or both).
\end{itemize}

% TODO: Define precise
% TODO: Define false-sharing

% --------------------------------------------------------------------------------------

\section{Related Work}\label{sec:background}
\Toolname{} primarily draws on techniques from existing race detectors and false sharing detectors.
This is appropriate because some forms of false-positives for race detectors are caused by false
sharing. That is, where race detectors focus on two threads concurrently accessing the same
data, false sharing detectors focus on two threads concurrently accessing different data, but
on the same cache line.

% --------------------------------------
\subsection{Race Detection}

Race detectors can be categorized according to the following parameters:
\begin{description}
\item[Approach] Static or dynamic. \hfill \\
Static race detectors analyze source code or binaries, typically performing interprocedural control-
and data-flow analyses. The benefit of static analysis is that they can inspect all program paths.

Dynamic race detectors monitor memory accesses for races. The benefit of this approach is that
races can be caught "in the act". The drawbacks of this approach is that most dynamic race detectors
are too slow to run in production environments, and therefore must be tested on simulated workloads.
If these workloads do not exercise all code paths than many races can be missed. Dynamic race detectors
can be further broken down into on-the-fly detectors or post-mortem detectors, which analyze memory
access logs.

\item[Model] Precise or imprecise.  \hfill \\
Precise race detectors report no false-positives and are usually implemented using Lamport's
happens-before relationship \cite{VectorClocks}. Precise race detectors often trade efficiency
for completeness in order to achieve their precision. That is, they sometime miss true races in
code.

Imprecise race detectors can have high false positive rates, but tend to be more complete in terms of
including races not detected by precise detectors. They tend to be implemented using a variant of the
lockset algorithm \cite{Eraser}.

\item[Granularity] Fine-grained or coarse-grained. \hfill \\
Fine-grained race detectors operate at or close to the granularity of memory accesses. Coarse-grained
race detectors operate at an object granularity, or at a fixed granularity (e.g. cache line, page).

The granularity of a race detector can affect its precision. For example, a happens-before race detector 
can be imprecise (i.e. report false positives) by operating at a coarse granularity. If the granularity were
the size of a cache line, then the race detector would be a cache line contention detector: it would hint
at races as well as false-sharing.

\item[Mode] Comprehensive or sampling-based. \hfill \\
Comprehensive race detectors typically analyze every memory read and write. They are usually implemented
by instrumenting all program code.

Sampling-based race detectors analyze either a subset of the code or a subset of the data. In the former
case, they focus on detecting races by specific code.
\end{description}

Under this categorization scheme, \Toolname{} is a dynamic, precise, variable-granularity, mixed-mode
false-sharing detector. It draws inspiration from a number of existing race detectors, including LiteRace
\cite{LiteRace}, MultiRace \cite{MultiRace}, RaceMob \cite{RaceMob}, RaceTrack \cite{RaceTrack}, and
DataCollider \cite{DataCollider}.

LiteRace \cite{LiteRace} is a precise, sampling-based dynamic race detector implemented on top of the Microsoft
Pheonix binary translator. It samples at a function granularity, and executes either one specialized version of the
code or another. The sampling rate for a function is inversely proportional to how often it executes. This is based
on the insight that races are likely to occur in infrequently executed code, because otherwise they are benign or
would have been noticed and resolved. This insight, called the ``cold code hypothesis", motivated \Toolname's
post-mortem ranking scheme: cases of false sharing are more highly ranked when one---but \emph{not} both---of
the participating threads are executing cold code. This variant on the cold code hypothesis emphasizes the similarly
hard-to-find case of cold code pessimizing hot code.

\newcommand\TypeIdPair{$\langle PC,size \rangle$ }

MultiRace \cite{MultiRace} is a comprehensive, dynamic race detector that employs a hybrid happens-before and
lockset-based approach. It operates at an object granularity, and therefore reports false-positives. It is unique
in that it uses ``pointer swizzing" as a means of remapping memory to ``minipages" that track the dominating
access pattern of a thread (read-write > read-only > no-access). \Toolname{} uses a similar pointer swizzing approach, but
in the form of address watchpoints \cite{AddressWatchpoints}. Pointers to heap-allocated objects are ``swizzled" to
include a type identifier that is unique to a \TypeIdPair{} pair. This improves the diagnosis of false-sharing,
because it allows \Toolname{} to distinguish between two threads accessing different fields of the same object, and
two threads accessing two nearby objects of the same (or different) types.

RaceMob \cite{RaceMob} and RaceTrack \cite{RaceTrack} are two hybrid happens-before and lockset-based race detectors.
RaceMob begins by statically analyzing source code to find potential races (lockset), then dynamically verifies those races
(happens-before). RaceTrack is a JIT-based race-detector for Microsoft's common language runtime (CLR). It uses the
concept of threadsets to efficiently maintain happens-before relationships for memory locations and detect the possible
presence of a race and uses lockets to confirm predicted races. The novelty of RaceTrack is that it adapts the detection
granularity from object-granularity to field-granularity. \Toolname{} follows the same high-level approach as both RaceMob
and RaceTrack in that it attempts to predict the source of false-sharing at one granularity, then verify those predictions 
at another granularity.

DataCollider \cite{DataCollider} is a dynamic, precise, sampling-based race detector for the kernel. The key strengths of
DataCollider is that it has low overhead, it is independent of the synchronization mechanisms being employed, and that
it often catches both threads in the act. DataCollider uniformly samples code by introducing breakpoints into the code.
When a breakpoint is hit, a hardware watchpoint is added to data that was about to be accessed by the interrupted instruction.
Then, the interrupted thread is paused for a short period of time to give another thread the opportunity to access the
same data, thus triggering the watchpoint and detecting a race. This brilliantly simple approach was the main inspiration
for \Toolname{}. \Toolname{} differs from DataCollider, in that the latter is code-centric insofar as it samples code, whereas
\Toolname{} is data-centric insofar as it samples data.

% --------------------------------------
\subsection{False Sharing Detection}

Similar to race detectors, false-sharing detectors can be sampling-based or comprehensive. Comprehensive approaches
can be categorized  into simulation-based and instrumentation-based approaches. Simulation-based approaches focus on
simulating the full cache hierarchy and can distinguish fine-grained events like false sharing due to set-associativity and
false sharing due to different data being accessed on the same cache line. Instrumentation-based approaches focus more
on mechanisms for estimating or detecting cache line contention.

\paragraph{Sampling-based}
Intel's VTune \cite{IntelVTune} an an imprecise, sampling-based profiler that can detect false sharing using hardware performance
counters and monitoring events. It represents the state-of-the-art in performance analysis tools and is able to provide detailed,
architecture-specific performance-tuning suggestions. Unlike \Toolname{}, VTune is unable to analyze only a portion of the code,
or only the portion of code that accesses specific shared data structures.

\paragraph{Comprehensive}

% Simulation based approaches:
% TODO: Callgrind/Cachegrind?
% TODO: Simics?
% TODO: Pluto?

\newcommand{\DrContention}{Zhao \emph{et al.}}

\DrContention{} \cite{DrContention} implement a dynamic false and true sharing detector using the DynamoRIO \cite{DynamoRIO}
DBT system and Umbra shadow memory framework \cite{Umbra}. Unlike \Toolname{}, \DrContention{} use fixed-granularity
shadow memory to track cache line ownership by processor cores. The drawback of this approach is that it necessitates checking
and updating the shadow memory on every memory access, which introduces its own form of cache line contention. \Toolname{}
avoids this issue by using shadow memory and tagged reads as a proxy for memory access behavior instead of as a meta-data store.

Plastic \cite{Plastic} is a Xen-based dynamic false sharing detector and repairer. Like \Toolname{}, it takes a pipeline-based
approach that successively narrows down on the root causes of false sharing. Plastic also uses DBT as part of its
repair mechanism. Plastic's pipeline begins by using hardware performance counters related to cache contention
events to detect the presence of false-sharing. Then, it applies a similar technique to MultiRace \cite{MultiRace}, in
that it tracks the dominant access pattern by each thread to memory on a shared page. This information informs
their repair mechanism, which transparently re-maps memory accesses at a byte granularity.

SHERIFF \cite{SHERIFF} is a false-sharing detector and repairer that works in a similar way to MultiRace and Plastic. Initially,
it applies read-only hardware page protection to all shared memory. Then, as threads attempt to write to protected pages, those
pages are privately mapped as read/write. When the same page is privately mapped by two or more threads, SHERIFF applies
the twinning and diffing approach of TreadMarks \cite{TreadMarks} to detect local changes to shared pages and narrow down
on instances of false sharing. Like \Toolname{}, SHERIFF uses a coarse-grained mechanism to hint at the potential for races.

% --------------------------------------------------------------------------------------
\section{Methodology}\label{sec:methodology}

This section introduces our approach to finding false sharing bugs. The goal of our approach is extract beliefs about
where false sharing might be, then iteratively strenghten those beliefs, either verifying or discarding them in the process.

We begin with the same intuition as SHERIFF: if false sharing occurs on a single cache line, then that implies that two
threads are simultaneously accessing the same page of memory \cite{SHERIFF}. Therefore, by detecting contention on
a single page, we can establish a belief that false sharing might occur on the contended page.

Unfortunately, a page-granularity detection of false sharing is na{\"i}ve: it provides no guidance as to the location of the
contended cache line, if any. Our insight is that we can narrow down on instances of false sharing by varying the
granularity of the mechanism used to detect thread contention. That is, we can start with a coarse-grained but
innacurate approach that tells us what code might cause false sharing, thus establishing an initial belief set. Then, we
can strengthen or discard beliefs by repeating this same process, but using finer-grained detection mechanisms that
apply only to code thought to contain false sharing.

The remainder of this section explains how we proxy memory access behavior (\Secref{proxymem}), how we use sampling
to detect contending accesses to shared memory (\Secref{heapsample}), how we reduce false negatives with our
sampling-based approach (\Secref{falsenegs}), how we achieve precision by eliminating false positives (\Secref{falsepos}),
and finally how we rank discovered instances of false sharing (\Secref{ranking})

% --------------------------------------
\subsection{Proxy Memory and Tagged Reads}\label{sec:proxymem}
Proxy memory is a form of variable-granularity shadow memory that proxies actual memory access behavior. The key
idea is to instrument all loads and stores in such a way that their load/store behavior is reflected in the shadow
memory in a way that can be externally observed. For example, one way to achieve this is to instrument every load
to read from shadow memory, every store to modify shadow memory, and trap loads and stores by adding hardware
watchpoints to shadow memory locations. Such heavyweight instrumentation would be inefficient, and so we apply
two optimizations: belief-based selective instrumentation, and tagged reads.

\paragraph{Selective Instrumentation}
The first optimization to proxy memory is to not instrument all code to operate on proxy memory. Initially, \Toolname{}
comprehensively instruments all code, thus incurring significant overheads. However, as beliefs about false sharing
are refined, less and less code is instrumented, thus allowing code to run faster when precision during identification of
potential false sharing is needed.

\paragraph{Tagged Reads}
The second optimization avoids the problem of introducing unnecessary cache coherency traffic by treating proxy memory
as read-only, and disambiguating the intention of an operation (load, store) by using a \emph{tagged read}. In our implementation,
a read operations (on the proxy memory) that intend to communicate loads (of actual memory) are distinguished from
read operations that intend to communicate a store by using different encodings of the logical \texttt{OR} instruction.

% --------------------------------------
\subsection{Heap Sampling}\label{sec:heapsample}

\Toolname{} follows DataCollider's sampling based approach, but samples proxy memory instead of code \cite{DataCollider}.
\Toolname{} applies roughly the following high-level algorithm to sample proxy memory and detect potential false sharing:
\begin{enumerate}
\item Choose a proxy memory address $P$ uniformly at random. Add a hardware watchpoint $w(P)$ to $P$.
\item If the watchpoint $w(P)$ is not triggered within $t$ units of time, then remove $w(P)$ and go to step (1).
\item If $w(P)$ is triggered for thread $T_i$ then pause $T_i$ for $t$ units of time. The hope is that within this
time slice, another thread, $T_j$ will be caught in the act of contending on $P$ by triggering $w(P)$.
\item If $w(P)$ is not triggered again for another $t$ time units, then remove $w(P)$ and go to step (1).
\item If another thread $T_j$ triggers $w(P)$, then record stack traces for $T_i$ and $T_j$, and mark the associated program
counters of each of $T_i$ and $T_j$ as potentially having false sharing.
\item Go to step (1) to repeat the process for a new sample location.
\end{enumerate}

As described, our sampling algorithm is susceptible to bias: if only a small percentage of the application's data is concurrently
modified, or ``hot", then our algorithm might miss potential false sharing by wasting time sampling from the cold data.
To avoid this issue, we apply two new techniques: \emph{uniform heap sampling}, and \emph{selective heap sampling}.

\newcommand\TypeId{$TypeId$}

\paragraph{Uniform Heap Sampling}
Uniform heap sampling is a round-robin algorithm that uniformly selects proxy memory locations to sample according to
the types of data proxied. We interpose on memory allocation and deallocation routines (e.g. \texttt{malloc} and \texttt{free})
and associate unique \TypeId{}s to each \TypeIdPair{}, where $PC$ is the return address of an allocation function, and $size$ is the
number of bytes allocated.

For each proxy memory location $P$, we maintain a counting \emph{typeset}: a mapping of \TypeId{}s to the number of objects of type
\TypeId{} that are fully or partially proxied by $P$.

\paragraph{Selective Heap Sampling}
Selective heap sampling is a biased variant of uniform heap sampling that allows a user to guide the sampler to only focus
on specific types of data. This is beneficial when one is trying to solve specific performance problems related to a subset
of the data being accessed by the application.

% --------------------------------------
\subsection{Reducing False Negatives}\label{sec:falsenegs}

One challenge with sampling-based approaches is their ability to get ``unlucky" and either not sample
the right thing, or sample the right thing at the wrong time. In our case, this results in false-negatives:
we might report that no false sharing is detected in some code, but this does not imply the absence of
false sharing.

To address this challenge, we propagate existing beliefs about potential false sharing on a single instruction
to nearby instructions, as well as to instructions that operate on data of the same type. The intuition is
two-fold. First, we assume locality of accesses to shared data: if we observe a shared access on one instruction,
then nearby instructions probably access shared data as well. Second, a belief of false sharing is a belief of
sharing: if we observe an object of a particular type being concurrently accessed in one place, then it might
imply that concurrent accesses can also happen in other places.

We perform two separate analyses to extend belief sets: \begin{description}
\item[Type Analysis] \hfill \\
This analysis instruments all code and identifies what types of objects are accessed by what code. We apply the same
notion of typesets as used in the uniform heap sampler; however, in this case typesets are associated with basic
blocks of code instead of with proxy memory. We use address watchpoints \cite{AddressWatchpoints} to taint
allocated addresses with their \TypeId{}, and then propagate these \TypeId{}s to the typesets of the blocks operating
of data of each type.

\item[Concurrency Analysis] \hfill \\
This analysis instruments all code and identifies what functions can execute concurrently. Each instrumented function
is associated with a core set and a threadset \cite{DrContention}. When executed, the coreset is updated to include the
identifier of the CPU core executing the function. Similarly, the function's threadset is updated to include an approximation
of the thread identifier ($SP >> 13$). We anlayse the coresets and threadsets of each function to determine whether or
not a function can execute concurrently (across CPU cores, or in two threads on the same core).

\end{description}

We combine the type and concurrency analyses to propagate beliefs in the following way: if sampling detects potential
false sharing on a single instruction operating on a specific type of object, then we mark all instructions in the intersection
of those identified to operate on the same type with the set of concurrent functions.

% --------------------------------------
\subsection{Eliminating False Positives}\label{sec:falsepos}

We apply two techniques to reduce and finally eliminate false positives:
\begin{enumerate}
\item We iteratively refine the granularity of proxy memory, starting from the page granularity and going down to the cache
line granularity. As the granularity is refined and potential false sharing continues to be detected, our confidence in beliefs
of false sharing improves.

\item We apply a precise data race detector \cite{DataCollider} to instructions that demonstrate cache line contention. This
allows us to distinguish between true and false sharing.
\end{enumerate}

Distinguishing between false sharing and pseudo sharing is more challenging. We approximate this detection by extending
the type analysis phase to record the relative byte offset within each type of accessed object for each instruction. If false
sharing is detected between two instructions operating on the same offset ($\pm$ the memory access size), then we report
pseudo sharing, otherwise false sharing. Another valid heuristic would include the natural alignment and sizes of object
types.

% TODO: How to make it precise? Apply DataCollider to places where we've found potential false sharing at the granularity
% of a cache line.

% How do we distinguish between false sharing and pseudo sharing? Use the type analysis feature to figure out what offsets
% into what data were being accessed.h

% --------------------------------------
\subsection{Ranking}\label{sec:ranking}

Because we sample data of all types, we might find instances of false sharing that have no significant impact on performance.
Therefore, we need a means of estimating the performance impact of false sharing so that we can rank detected instances of
false sharing from most to least important. We accomplish this as part of a separate analysis step. This step
instruments all code to count how often each basic block is executed. We correlate this information with detected
instances of false sharing to infer cases where one or both threads are executing hot code and contending on a single
cache line. In the former case, we apply a variant of the cold code hypothesis \cite{LiteRace} to rank cases of cold
code ``pessimizing" hot code more highly.

%At a high-level, \Toolname{} follows the same principle as Plastic \cite{Plastic} and SHERIFF \cite{SHERIFF}: it detects
%false sharing at a coarse granularity and then verifies detected cases at a finer granularity. Unlike other approaches,
%\Toolname{} repeats this process by incrementally refining the detection granularity, and by selectively instrumenting only
%that code that appears to trigger false sharing. Whereas Plastic and SHERIFF use page-granularity detection, \Toolname{}
%is able to vary the granularity over time using a \emph{proxy memory} (\Secref{proxymem}). 

% --------------------------------------------------------------------------------------
\section{Status}\label{sec:status}
\Toolname{} is being implemented using the Granary dynamic binary translator \cite{Granary} for the x86-64 architecture.

A limited form of proxy memory has been implemented. Work is progressing on making it possible to generically interpose
on memory allocators. Once this is done, the next step will be to work on the type analysis, as well as to associate typesets
with proxy memory locations. Code hotness analysis is expected to be an easy-to-do task. Concurrency analysis will be trickier.

Sampling and hardware watchpoint management will be orchestrated using GDB scripts.

% TODO...

% --------------------------------------------------------------------------------------
\section{Evaluation}\label{sec:evaluation}
Our evaluation aims to answer the following questions:
\begin{itemize}
\item What are \Toolname{}'s overheads? For example, what was the performance impact of the selective instrumentation
and tagged read optimizations?

\item How sensitive is \Toolname{} to different sampling rates, delays, and approaches? How does the thread
pause time during sampling affect our ability to detect false sharing? Can we quantify the benefits (if any) of
uniform heap sampling over na{\"i}ve heap sampling?

\item How effective is \Toolname{} at discovering false sharing? Is it able to distinguish between true sharing, false sharing,
and pseudo sharing?

\item How impactful are the instances of false sharing detected by \Toolname{}? Is code hotness a good metric for ranking
false sharing?
\end{itemize}

\subsection{Evaluation Challenges}
Granary is a medium weight DBT system, therefore its overheads will likely be significant. It is also new, and so no time has been
spent optimizing the code produced by Granary. Therefore, the overheads are not expected to be particularly good. The major
challenge will be convincing readers that:
\begin{enumerate}
\item Most of the analysis can be performed in a non-production environment, where we can claim that speed doesn't matter.
This will obviously be a contentious claim, because speed always matters, especially in false sharing detection. Here, I hope
that the belief propagation steps of the type and concurrency analysis stages will be beneficial in allaying those concerns.

\item The application of DataCollider to distinguish true and false sharing can be performed in production given the results of
all prior stages of the pipeline.
\end{enumerate}


% --------------------------------------------------------------------------------------
\bibliographystyle{abbrv}
\bibliography{library}
\end{document}


